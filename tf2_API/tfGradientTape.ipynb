{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python361064bitenvgpuconda37aa1c40388546da8d8fab64bfb48375",
   "display_name": "Python 3.6.10 64-bit ('env_gpu': conda)"
  }
 },
 "cells": [
  {
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)]\n",
    ")\n",
    "\n",
    ""
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.0.0\n"
    }
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "source": [
    "\n",
    "def gradient_test():\n",
    "    #-------------------一元梯度案例---------------------------\n",
    "    print(\"一元梯度\")\n",
    "    x=tf.constant(value=3.0)\n",
    "    with tf.GradientTape(persistent=True,watch_accessed_variables=True) as tape:\n",
    "        tape.watch(x)\n",
    "        y1=2*x\n",
    "        y2=x*x+2\n",
    "        y3=x*x+2*x\n",
    "    #一阶导数\n",
    "    dy1_dx=tape.gradient(target=y1,sources=x)\n",
    "    dy2_dx = tape.gradient(target=y2, sources=x)\n",
    "    dy3_dx = tape.gradient(target=y3, sources=x)\n",
    "    print(\"dy1_dx:\",dy1_dx)\n",
    "    print(\"dy2_dx:\", dy2_dx)\n",
    "    print(\"dy3_dx:\", dy3_dx)\n",
    "\n",
    "\n",
    "    # # -------------------二元梯度案例---------------------------\n",
    "    print(\"二元梯度\")\n",
    "    x = tf.constant(value=3.0)\n",
    "    y = tf.constant(value=2.0)\n",
    "    with tf.GradientTape(persistent=True,watch_accessed_variables=True) as tape:\n",
    "        tape.watch([x,y])\n",
    "        z1=x*x*y+x*y\n",
    "    # 一阶导数\n",
    "    dz1_dx=tape.gradient(target=z1,sources=x)\n",
    "    dz1_dy = tape.gradient(target=z1, sources=y)\n",
    "    dz1_d=tape.gradient(target=z1,sources=[x,y])\n",
    "    print(\"dz1_dx:\", dz1_dx)\n",
    "    print(\"dz1_dy:\", dz1_dy)\n",
    "    print(\"dz1_d:\",dz1_d)\n",
    "    print(\"type of dz1_d:\",type(dz1_d))\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    gradient_test()\n",
    "\n",
    ""
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "一元梯度\ndy1_dx: tf.Tensor(2.0, shape=(), dtype=float32)\ndy2_dx: tf.Tensor(6.0, shape=(), dtype=float32)\ndy3_dx: tf.Tensor(8.0, shape=(), dtype=float32)\n二元梯度\ndz1_dx: tf.Tensor(14.0, shape=(), dtype=float32)\ndz1_dy: tf.Tensor(12.0, shape=(), dtype=float32)\ndz1_d: [<tf.Tensor: id=117, shape=(), dtype=float32, numpy=14.0>, <tf.Tensor: id=118, shape=(), dtype=float32, numpy=12.0>]\ntype of dz1_d: <class 'list'>\n"
    }
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "根据这个例子说一下tf.GradientTape这个类的常见的属性和函数，更多的可以去官方文档来看。\n",
    "\n",
    "__init__(persistent=False,watch_accessed_variables=True)\n",
    "\n",
    "作用：创建一个新的GradientTape\n",
    "参数:\n",
    "\n",
    "    persistent: 布尔值，用来指定新创建的gradient tape是否是可持续性的。默认是False，意味着只能够调用一次gradient（）函数。\n",
    "    watch_accessed_variables: 布尔值，表明这个gradien tap是不是会自动追踪任何能被训练（trainable）的变量。默认是True。要是为False的话，意味着你需要手动去指定你想追踪的那些变量。\n",
    "\n",
    "比如在上面的例子里面，新创建的gradient tape设定persistent为True，便可以在这个上面反复调用gradient（）函数。\n",
    "\n",
    "watch(tensor)\n",
    "作用：确保某个tensor被tape追踪\n",
    "\n",
    "参数:\n",
    "\n",
    "    tensor: 一个Tensor或者一个Tensor列表\n",
    "\n",
    "gradient(target,sources,output_gradients=None,unconnected_gradients=tf.UnconnectedGradients.NONE)\n",
    "作用：根据tape上面的上下文来计算某个或者某些tensor的梯度\n",
    "参数:\n",
    "\n",
    "    target: 被微分的Tensor或者Tensor列表，你可以理解为经过某个函数之后的值\n",
    "    sources: Tensors 或者Variables列表（当然可以只有一个值）. 你可以理解为函数的某个变量\n",
    "    output_gradients: a list of gradients, one for each element of target. Defaults to None.\n",
    "    unconnected_gradients: a value which can either hold ‘none’ or ‘zero’ and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in ‘UnconnectedGradients’ and it defaults to ‘none’.\n",
    "\n",
    "返回:\n",
    "一个列表表示各个变量的梯度值，和source中的变量列表一一对应，表明这个变量的梯度。\n",
    "\n",
    "上面的例子中的梯度计算部分可以更直观的理解这个函数的用法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "有关梯度下降算法的总结 https://ruder.io/optimizing-gradient-descent/"
   ]
  }
 ]
}