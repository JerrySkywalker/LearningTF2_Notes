# 机器学习基础

本节介绍机器学习的一些基础概念

## 1.学习算法

### 1.1基础概念

- 学习：对于某类任务$T$和性能度量$P$，经过经验$E$的改进，$T,P$会有一定程度的提升
- 任务：即我们要实现的具体目标
  - 描述任务的方式
    - 样本：特征的集合
    - 特征
  - 常见的任务种类
    - 分类
    - 输入缺失分类
    - 回归
    - 转录
    - 机器翻译
    - 结构化输出(eg.语法分析)
    - 异常检测
    - 合成和采样
    - 缺失值填补
    - 去噪
    - 密度估计和质量函数估计
- 性能度量：评估任务完成情况的函数
  - 常见的性能度量
    - 准确率
    - 错误率
  - 使用测试集
  - 设计取决于应用(指标粗粒度/细粒度?惩罚程度?)
  - 可采用替代标准或良好近似
- 经验E
  - 无监督学习：学习出数据集的有用的结构性质(显式或隐式)
  - 监督学习：通过数据集上的标签，推测分类标准
  - 区别：无监督学习得到$p(x)$，监督学习得出$p(y|x)$
  - 监督学习和无监督学习可以相互转化(全概率公式、贝叶斯公式)
  - 设计矩阵：数据集的常用表示方法。设计矩阵的每一行包含一个不同的样本，每一列代表不同的特征。

### 1.2实例：线性回归

线性回归是一类最常见的，也是最简单的学习算法的使用领域，其有助于我们回顾整个学习算法的来龙去脉。

#### 1.2.1一般形式

一般的线性回归模型如下：
$$
\hat{y}=w^{\top} x+b
$$

- $w$：权重
- $b$：偏置
- $y(x)$：这是一个从特征到预测的一个映射，称为仿射函数

### 1.2.2 度量

假设我们有一个设计矩阵，不用它来训练，而仅用它来评估性能，我们称之为测试集。

一种常用方式是计算测试集上的均方误差(Mean Squared Error,MSE)
$$
\text { MSE }_{\text {test }}=\frac{1}{m} \sum_{i}\left(\hat{y}^{(\text {test })}-y^{(\text {test })}\right)_{i}^{2}
$$
显然，
$$
\mathrm{MSE}_{\text {test }}=\frac{1}{m}\left\|\hat{y}^{(\text {test })}-y^{(\text {test })}\right\|_{2}^{2}
$$

构建一个机器学习算法的思路如下：通过观察训练集$\left(X^{(\text {train })}, y^{(\text {train })}\right)$获得经验，减少$\operatorname{MSE}_{\text {test}}$，以改进权重$w$。

减少$MSE_{test}$，我们可以简化模型，仅理解为求其导数为零的情况，以下进行一些推导：
$$
\begin{aligned} \nabla_{w} \operatorname{MSE}_{\text {train }}=0 \\ \Rightarrow \nabla_{w} \frac{1}{m}\left\|\hat{y}^{(\text {train })}-y^{(\text {train })}\right\|_{2}^{2}=0 \\ \Rightarrow \frac{1}{m} \nabla_{w}\left\|X^{(\text {train })} w-y^{(\text {train })}\right\|_{2}^{2}=0 \\ \Rightarrow \nabla_{w}\left(X^{(\text {train })} w-y^{(\text {train })}\right)^{\top}\left(X^{(\text {train })} w-y^{(\text {train })}\right)=0 \\ \Rightarrow 2 X^{(\text {train })} w-2 w^{\top} X^{(\text {train }) \top} y^{(\text {train }) \top} y^{(\text {train })}=0 \\ \Rightarrow w=\left(X^{(\text {train })}\right)^{-1} X^{(\text {train }) \top} y^{(\text {train })} \end{aligned}
$$

由此推到的$w$称为正规方程。

## 2.为何能够“学习”：容量，过拟合和欠拟合

机器学习的主要挑战是：算法能够在先前未观测到新输入上表现良好。这种能力称为泛化能力。

这牵扯到两种误差：

- 训练误差：在训练集上使用，我们训练时的目的是降低训练误差
- 泛化误差/测试误差：我们真正关心的误差，即在面对未观测过的输入时，做出的判断误差尽量小

统计学习理论研究的就是这两种误差之间的关系。其告诉我们一个简单的道理：如果训练集和测试集是随意收集的，我们能够做的很有限；但如果对于训练集和测试集的收集方式有一定的假设，我们能够得到一些有趣的结论。

### 2.1 统计学习理论的基础概念

- 数据生成过程：训练集和测试集通过被称为~的概率分布生产
- 独立同分布假设
  - 相互独立
  - 同分布
- 数据生成分布
  - 在独立同分布的情况下，数据集和测试集可以共享这个潜在分布，称为数据生成分布

数据生成分布和独立同分布假设允许我们在数学上研究训练误差和测试误差之间的联系。

### 2.2 评价学习算法的两个依据和异常状态

评价学习算法的两个依据是：

1. 降低训练误差
2. 缩小训练误差和测试误差之间的差距

与两个评价标准分别对应，我们定义两个学习算法效果不佳时的状态：

1. 欠拟合：模型不能再训练集上获得足够低的误差
2. 过拟合：训练误差和测试误差之间的差距过大

### 2.3 容量和假设空间

- 容量：通俗来讲，容量指的是其拟合函数的能力。改变容量额方法有很多，例如多项式次数，参数个数等。容量不仅取决于模型的选择。
  - 表示容量：模型规定了可以从那些函数族中选择函数
  - 有效容量：一个可以大大降低训练误差的函数族的容量。有时，有效容量小于表示容量
  - 最优容量：若小于最优容量太多时是欠拟合的，若大于最优容量太多时是过拟合的
- 假设空间：与容量相对应，即学习算法可以选择未解决方案的函数集。

在表示容量中找到一个完美函数往往是困难的，一般我们退而求其次，选择在有效容量中找一个效果较好的函数。

这说明：泛化是区别最优化和机器学习的最本质区别。

### 2.4 合适的容量？几条原则

- 奥卡姆剃刀原则：“在同样能够解释一个现象的理论中，选那个最简单的”。这条原则早在托勒密时代就被提出，在20世纪被概率论创始人形式化和精确化。
- 量化模型的容量
  - Vapnik-Chervonenkis维度(VC维)：VC维度量二元分类器的容量，最有名的量化模型的容量的方法之一。
  - 量化模型的容量使得统计学理论可以量化预测
    - 重要结论：训练误差和泛化误差之间差异的上界随着模型容量的增长而增长，随着训练样本的增多而下降。其实际关系类似一个u型曲线，这意味着其存在一个最优容量
  - 对于学习算法的指导有限，所以应用相对较少

对于复杂的问题，我们往往无法去确定最恰当的模型容量。为了充分性，我们往往选择模型容量大在训练集上进行训练。显然此时过拟合问题便成了我们主要需要克服的问题。

### 2.5 没有免费午餐定理

    以下参考：    https://blog.csdn.net/u013238941/article/details/79091252

定理的文字表述有以下几个版本：

- 若对于某些问题算法$L_a$学得的模型更好，那么必然存在另一些问题，这里算法$L_b$学得的模型更好
- 在所有数据生成分布平均之后，每个分类算法在未事观测的点上都有着相同的错误率

其数学表述为：
$$
E_{o t e}\left(L_{a} | X, f\right)=\sum_{h} \sum_{x \in \chi-X} P(x) \mathbb{I}(h(x)=f(x)) P\left(h | X, L_{a}\right)
$$

- $E_{ote}$：训练集外误差(off-training error,ote)的期望
- $\chi$：样本空间(样本的属性张成的空间)
- $H$：假设空间
- $L_a$：学习算法。学习算法有其偏好性，对于相同的训练数据，不同的学习算法可以产生不同的假设，学得不同的模型，因此才会有那个学习算法对于具体问题更好的问题
- $P\left(h | X, L_{a}\right)$：算法$L_a$​基于训练数据$X$产生假设$h$的概率
- $f$：代表希望学得的真实目标函数，要注意这个函数也不是唯一的，而是存在一个函数空间，在这个空间中按某个概率分布

显然，期望的表达式中关于没有具体算法的，所以是算法是与期望无关，这就是没有免费午餐定理。

没有免费午餐定理暗示我们必须在特定任务上设计机器学习算法。我们应当建立一种偏好来达到这种要求，而当这种偏好和我们希望解决的学习问题相吻合时效果更好

### 2.6 正则化

类似于奥卡姆剃刀的原则（并非只有这一种原则，比如引入一些先验知识，或者说奥卡姆就是先验知识的一种）可以由模型的结构直接实现（通过改变模型的结构，使模型最后的解趋于简单化），我们称之为模型的归纳偏好。(inductive bias)

而人为的给模型编入先验知识，即增加归纳偏好的过程称为正则化

- 正则化：修改学习模型，使其降低泛化误差而非训练误差
  - 权重衰减：线性回归问题常用方法

## 5.3 超参数和验证集

- 超参数：超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。
- 验证集：用于挑选超参数的数据子集称为验证集