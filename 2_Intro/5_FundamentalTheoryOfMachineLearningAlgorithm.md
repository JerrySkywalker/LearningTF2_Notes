# 机器学习基础

本节介绍机器学习的一些基础概念

## 1.学习算法

### 1.1基础概念

- 学习：对于某类任务$T$和性能度量$P$，经过经验$E$的改进，$T,P$会有一定程度的提升
- 任务：即我们要实现的具体目标
  - 描述任务的方式
    - 样本：特征的集合
    - 特征
  - 常见的任务种类
    - 分类
    - 输入缺失分类
    - 回归
    - 转录
    - 机器翻译
    - 结构化输出(eg.语法分析)
    - 异常检测
    - 合成和采样
    - 缺失值填补
    - 去噪
    - 密度估计和质量函数估计
- 性能度量：评估任务完成情况的函数
  - 常见的性能度量
    - 准确率
    - 错误率
  - 使用测试集
  - 设计取决于应用(指标粗粒度/细粒度?惩罚程度?)
  - 可采用替代标准或良好近似
- 经验E
  - 无监督学习：学习出数据集的有用的结构性质(显式或隐式)
  - 监督学习：通过数据集上的标签，推测分类标准
  - 区别：无监督学习得到$p(x)$，监督学习得出$p(y|x)$
  - 监督学习和无监督学习可以相互转化(全概率公式、贝叶斯公式)
  - 设计矩阵：数据集的常用表示方法。设计矩阵的每一行包含一个不同的样本，每一列代表不同的特征。

### 1.2实例：线性回归

线性回归是一类最常见的，也是最简单的学习算法的使用领域，其有助于我们回顾整个学习算法的来龙去脉。

#### 1.2.1一般形式

一般的线性回归模型如下：
$$
\hat{y}=w^{\top} x+b
$$

- $w$：权重
- $b$：偏置
- $y(x)$：这是一个从特征到预测的一个映射，称为仿射函数

### 1.2.2 度量

假设我们有一个设计矩阵，不用它来训练，而仅用它来评估性能，我们称之为测试集。

一种常用方式是计算测试集上的均方误差(Mean Squared Error,MSE)
$$
\text { MSE }_{\text {test }}=\frac{1}{m} \sum_{i}\left(\hat{y}^{(\text {test })}-y^{(\text {test })}\right)_{i}^{2}
$$
显然，
$$
\mathrm{MSE}_{\text {test }}=\frac{1}{m}\left\|\hat{y}^{(\text {test })}-y^{(\text {test })}\right\|_{2}^{2}
$$

构建一个机器学习算法的思路如下：通过观察训练集$\left(X^{(\text {train })}, y^{(\text {train })}\right)$获得经验，减少$\operatorname{MSE}_{\text {test}}$，以改进权重$w$。

减少$MSE_{test}$，我们可以简化模型，仅理解为求其导数为零的情况，以下进行一些推导：
$$
\begin{aligned} \nabla_{w} \operatorname{MSE}_{\text {train }}=0 \\ \Rightarrow \nabla_{w} \frac{1}{m}\left\|\hat{y}^{(\text {train })}-y^{(\text {train })}\right\|_{2}^{2}=0 \\ \Rightarrow \frac{1}{m} \nabla_{w}\left\|X^{(\text {train })} w-y^{(\text {train })}\right\|_{2}^{2}=0 \\ \Rightarrow \nabla_{w}\left(X^{(\text {train })} w-y^{(\text {train })}\right)^{\top}\left(X^{(\text {train })} w-y^{(\text {train })}\right)=0 \\ \Rightarrow 2 X^{(\text {train })} w-2 w^{\top} X^{(\text {train }) \top} y^{(\text {train }) \top} y^{(\text {train })}=0 \\ \Rightarrow w=\left(X^{(\text {train })}\right)^{-1} X^{(\text {train }) \top} y^{(\text {train })} \end{aligned}
$$

由此推到的$w$称为正规方程。

## 2.为何能够“学习”：容量，过拟合和欠拟合

机器学习的主要挑战是：算法能够在先前未观测到新输入上表现良好。这种能力称为泛化能力。

这牵扯到两种误差：

- 训练误差：在训练集上使用，我们训练时的目的是降低训练误差
- 泛化误差/测试误差：我们真正关心的误差，即在面对未观测过的输入时，做出的判断误差尽量小

统计学习理论研究的就是这两种误差之间的关系。其告诉我们一个简单的道理：如果训练集和测试集是随意收集的，我们能够做的很有限；但如果对于训练集和测试集的收集方式有一定的假设，我们能够得到一些有趣的结论。

### 2.1 统计学习理论的基础概念

- 数据生成过程：训练集和测试集通过被称为~的概率分布生产
- 独立同分布假设
  - 相互独立
  - 同分布
- 数据生成分布
  - 在独立同分布的情况下，数据集和测试集可以共享这个潜在分布，称为数据生成分布

数据生成分布和独立同分布假设允许我们在数学上研究训练误差和测试误差之间的联系。

### 2.2 评价学习算法的两个依据和异常状态

评价学习算法的两个依据是：

1. 降低训练误差
2. 缩小训练误差和测试误差之间的差距

与两个评价标准分别对应，我们定义两个学习算法效果不佳时的状态：

1. 欠拟合：模型不能再训练集上获得足够低的误差
2. 过拟合：训练误差和测试误差之间的差距过大

### 2.3 容量和假设空间

- 容量：通俗来讲，容量指的是其拟合函数的能力。改变容量额方法有很多，例如多项式次数，参数个数等。容量不仅取决于模型的选择。
  - 表示容量：模型规定了可以从那些函数族中选择函数
  - 有效容量：一个可以大大降低训练误差的函数族的容量。有时，有效容量小于表示容量
- 假设空间：与容量相对应，即学习算法可以选择未解决方案的函数集。

在表示容量中找到一个完美函数往往是困难的，一般我们退而求其次，选择在有效容量中找一个效果较好的函数。
